
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>&lt;no title&gt; &#8212; Sanstyle Demo</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/page.css" />
    <link rel="stylesheet" type="text/css" href="../_static/w3css/4/w3.css" />
    <link rel="stylesheet" type="text/css" href="../_static/w3css/4/w3pro.css" />
    <link rel="stylesheet" type="text/css" href="../_static/w3css/4/w3mobile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/xin-css/main.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/tabs.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "SanstyleLab/pytorch-book");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "üí¨ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script src="../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://SanstyleLab.github.io/sanstyle-starter/use/01_train-512-face.html" />
    <link rel="shortcut icon" href="../_static/page-logo.jfif"/>
    <link rel="index" title="Á¥¢Âºï" href="../genindex.html" />
    <link rel="search" title="ÊêúÁ¥¢" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="zh_CN">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Sanstyle Demo</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ÊïôÁ®ã
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../start/index.html">
   Âø´ÈÄü‰∏äÊâã
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../start/00_extract.html">
     Ëß£ÂéãÊï∞ÊçÆÈõÜ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../start/01_dataset.html">
     Â§ÑÁêÜÊï∞ÊçÆ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../start/02_train.html">
     Ê®°ÂûãÂÆö‰πâ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../start/02_train2.html">
     Ê®°ÂûãÂÆö‰πâ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../start/03_dataset-mix.html">
     Â§ÑÁêÜÊï∞ÊçÆ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../start/raw.html">
     <code class="docutils literal notranslate">
      <span class="pre">
       rcParams
      </span>
     </code>
     &amp;
     <code class="docutils literal notranslate">
      <span class="pre">
       cycler
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ê∑∑Ê≤å
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../about/index.html">
   ÂÖ≥‰∫é
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../about/architecture.html">
     È°πÁõÆÊû∂ÊûÑ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../about/how.html">
     Â¶Ç‰ΩïÂêØÂä®È°πÁõÆ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../about/zreferences.html">
     ÂèÇËÄÉÊñáÁåÆ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../CHANGELOG.html">
     ÂèòÊõ¥Êó•Âøó
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://xinetzone.github.io/sanstyle-book/">
     Sanstyle Book
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <div class="w3-padding w3-card-4 w3-pale-green">
  <a href="https://github.com/xinetzone" class="tooltipped" target="_blank" data-tooltip="ËÆøÈóÆÊàëÁöÑGitHub"
      data-position="top" data-delay="50">
      <i class="fab fa-github"></i>
  </a>
  <a href="mailto:q735613050@163.com" class="tooltipped" target="_blank" data-tooltip="ÈÇÆ‰ª∂ËÅîÁ≥ªÊàë" data-position="top"
      data-delay="50">
      <i class="fas fa-envelope-open"></i>
  </a>
  <a href="tencent://AddContact/?fromId=50&amp;fromSubId=1&amp;subcmd=all&amp;uin=735613050" class="tooltipped"
      target="_blank" data-tooltip="QQËÅîÁ≥ªÊàë: 735613050" data-position="top" data-delay="50">
      <i class="fab fa-qq"></i>
  </a>
  <a href="https://www.zhihu.com/people/liu-xin-wei-55" class="tooltipped" target="_blank"
      data-tooltip="ÂÖ≥Ê≥®ÊàëÁöÑÁü•‰πé: liu-xin-wei-55" data-position="top" data-delay="50">
      <i class="fab fa-zhihu1">Áü•</i>
  </a>
  <a target="_blank" rel="noopener" href="https://www.linkedin.com/in/xinet" class="tooltipped"
      data-tooltip="È¢ÜËã±ËÅîÁ≥ªÊàë: xinet" data-position="top" data-delay="50">
      <i class="fab fa-linkedin"></i>
  </a>
  <div><a href="https://github.com/xinetzone/sanstyle">‰∏äÂñÑËã•Ê∞¥</a> ÁâàÊùÉÊâÄÊúâ</div>
</div>

</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/use/01_train-512-face.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/SanstyleLab/pytorch-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/SanstyleLab/pytorch-book/issues/new?title=Issue%20on%20page%20%2Fuse/01_train-512-face.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/SanstyleLab/pytorch-book/main?urlpath=tree/docs/use/01_train-512-face.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/SanstyleLab/pytorch-book/blob/main/docs/use/01_train-512-face.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="simple visible nav section-nav flex-column">
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">../../</span><span class="n">apps</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>e:\kaggle\pytorch-book\apps
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">save_image</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span> 

<span class="kn">from</span> <span class="nn">tools.file</span> <span class="kn">import</span> <span class="n">mkdir</span>
<span class="kn">from</span> <span class="nn">utils.torch_loader_all</span> <span class="kn">import</span> <span class="n">Loader</span>
<span class="kn">from</span> <span class="nn">tools.toml</span> <span class="kn">import</span> <span class="n">load_option</span>
<span class="kn">from</span> <span class="nn">app</span> <span class="kn">import</span>  <span class="n">init</span><span class="p">,</span> <span class="n">mask_op</span><span class="p">,</span> <span class="n">array2image</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span> <span class="o">=</span> <span class="n">load_option</span><span class="p">(</span><span class="s1">&#39;../origin/train-512-face.toml&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">opt</span><span class="p">)</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">Loader</span><span class="p">(</span><span class="o">**</span><span class="n">opt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;root&#39;: &#39;E:/kaggle/datasets/CelebA/Img/img_align_celeba&#39;, &#39;mask&#39;: &#39;D:/kaggle/dataset/mask/testing_mask_dataset&#39;, &#39;fine_size&#39;: 512, &#39;batch_size&#39;: 1}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;CSA-512-face&#39;</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">base_opt</span> <span class="o">=</span> <span class="n">load_option</span><span class="p">(</span><span class="s1">&#39;../options/base512.toml&#39;</span><span class="p">)</span>
<span class="n">model_opt</span> <span class="o">=</span> <span class="n">load_option</span><span class="p">(</span><span class="s1">&#39;../options/train-new.toml&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">init</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">model_opt</span><span class="p">,</span> <span class="n">base_opt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>initialize network with normal
initialize network with normal
initialize network with normal
initialize network with normal
---------- Networks initialized -------------
UnetGeneratorCSA(
  (model): UnetSkipConnectionBlock_3(
    (model): Sequential(
      (0): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): UnetSkipConnectionBlock_3(
        (model): Sequential(
          (0): LeakyReLU(negative_slope=0.2, inplace=True)
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(3, 3), dilation=(2, 2))
          (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (3): LeakyReLU(negative_slope=0.2, inplace=True)
          (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (6): UnetSkipConnectionBlock_3(
            (model): Sequential(
              (0): LeakyReLU(negative_slope=0.2, inplace=True)
              (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(3, 3), dilation=(2, 2))
              (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (3): LeakyReLU(negative_slope=0.2, inplace=True)
              (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (5): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (6): CSA(
                (model): Sequential(
                  (0): LeakyReLU(negative_slope=0.2, inplace=True)
                  (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(3, 3), dilation=(2, 2))
                  (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (3): LeakyReLU(negative_slope=0.2, inplace=True)
                  (4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (5): CSA_model(threshold: 0.3125 ,triple_weight 1)
                  (6): InnerCos(skip: True ,strength: 1)
                  (7): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (8): UnetSkipConnectionBlock_3(
                    (model): Sequential(
                      (0): LeakyReLU(negative_slope=0.2, inplace=True)
                      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(3, 3), dilation=(2, 2))
                      (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                      (3): LeakyReLU(negative_slope=0.2, inplace=True)
                      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                      (5): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                      (6): UnetSkipConnectionBlock_3(
                        (model): Sequential(
                          (0): LeakyReLU(negative_slope=0.2, inplace=True)
                          (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(3, 3), dilation=(2, 2))
                          (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                          (3): LeakyReLU(negative_slope=0.2, inplace=True)
                          (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                          (5): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                          (6): UnetSkipConnectionBlock_3(
                            (model): Sequential(
                              (0): LeakyReLU(negative_slope=0.2, inplace=True)
                              (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(3, 3), dilation=(2, 2))
                              (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                              (3): LeakyReLU(negative_slope=0.2, inplace=True)
                              (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                              (5): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                              (6): UnetSkipConnectionBlock_3(
                                (model): Sequential(
                                  (0): LeakyReLU(negative_slope=0.2, inplace=True)
                                  (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(3, 3), dilation=(2, 2))
                                  (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                                  (3): LeakyReLU(negative_slope=0.2, inplace=True)
                                  (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                                  (5): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                                  (6): UnetSkipConnectionBlock_3(
                                    (model): Sequential(
                                      (0): LeakyReLU(negative_slope=0.2, inplace=True)
                                      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(3, 3), dilation=(2, 2))
                                      (2): ReLU(inplace=True)
                                      (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                                      (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                                    )
                                  )
                                  (7): ReLU(inplace=True)
                                  (8): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                                  (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                                  (10): ReLU(inplace=True)
                                  (11): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                                  (12): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                                )
                              )
                              (7): ReLU(inplace=True)
                              (8): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                              (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                              (10): ReLU(inplace=True)
                              (11): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                              (12): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                            )
                          )
                          (7): ReLU(inplace=True)
                          (8): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                          (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                          (10): ReLU(inplace=True)
                          (11): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                          (12): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                        )
                      )
                      (7): ReLU(inplace=True)
                      (8): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                      (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                      (10): ReLU(inplace=True)
                      (11): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                      (12): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    )
                  )
                  (9): InnerCos2(skip: True ,strength: 1)
                  (10): ReLU(inplace=True)
                  (11): ConvTranspose2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (12): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (13): ReLU(inplace=True)
                  (14): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                  (15): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (7): ReLU(inplace=True)
              (8): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (9): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (10): ReLU(inplace=True)
              (11): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              (12): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            )
          )
          (7): ReLU(inplace=True)
          (8): ConvTranspose2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (9): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (10): ReLU(inplace=True)
          (11): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (12): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        )
      )
      (2): ReLU(inplace=True)
      (3): ConvTranspose2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)
Total number of parameters: 77692291
UnetGenerator(
  (model): UnetSkipConnectionBlock(
    (model): Sequential(
      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): UnetSkipConnectionBlock(
        (model): Sequential(
          (0): LeakyReLU(negative_slope=0.2, inplace=True)
          (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (3): UnetSkipConnectionBlock(
            (model): Sequential(
              (0): LeakyReLU(negative_slope=0.2, inplace=True)
              (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (3): UnetSkipConnectionBlock(
                (model): Sequential(
                  (0): LeakyReLU(negative_slope=0.2, inplace=True)
                  (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                  (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (3): UnetSkipConnectionBlock(
                    (model): Sequential(
                      (0): LeakyReLU(negative_slope=0.2, inplace=True)
                      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                      (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                      (3): UnetSkipConnectionBlock(
                        (model): Sequential(
                          (0): LeakyReLU(negative_slope=0.2, inplace=True)
                          (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                          (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                          (3): UnetSkipConnectionBlock(
                            (model): Sequential(
                              (0): LeakyReLU(negative_slope=0.2, inplace=True)
                              (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                              (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                              (3): UnetSkipConnectionBlock(
                                (model): Sequential(
                                  (0): LeakyReLU(negative_slope=0.2, inplace=True)
                                  (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                                  (2): ReLU(inplace=True)
                                  (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                                  (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                                )
                              )
                              (4): ReLU(inplace=True)
                              (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                              (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                            )
                          )
                          (4): ReLU(inplace=True)
                          (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                          (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                        )
                      )
                      (4): ReLU(inplace=True)
                      (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                      (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                    )
                  )
                  (4): ReLU(inplace=True)
                  (5): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
                  (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (4): ReLU(inplace=True)
              (5): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            )
          )
          (4): ReLU(inplace=True)
          (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        )
      )
      (2): ReLU(inplace=True)
      (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (4): Tanh()
    )
  )
)
Total number of parameters: 54419459
NLayerDiscriminator(
  (model): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))
    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))
  )
)
Total number of parameters: 2766529
PFDiscriminator(
  (model): Sequential(
    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (3): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  )
)
Total number of parameters: 10487296
-----------------------------------------------
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Ë∂ÖÂèÇÊï∞ËÆæÂÆö</span>
<span class="c1">## Âõ∫ÂÆöÂèÇÊï∞</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">display_freq</span> <span class="o">=</span> <span class="mi">49</span>
<span class="n">save_epoch_freq</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">## Ê®°ÂûãÂèÇÊï∞</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span>


<span class="n">model_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;CSA-crop-</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">-</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">&#39;</span>
<span class="n">image_save_dir</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">save_dir</span> <span class="o">/</span> <span class="s1">&#39;images&#39;</span>
<span class="n">mkdir</span><span class="p">(</span><span class="n">image_save_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ËÆ≠ÁªÉÈò∂ÊÆµ</span>
<span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">iter_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="n">epoch_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">epoch_iter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">epoch_iter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># ÂàùÂßãÂåñÊï∞ÊçÆÈõÜ</span>
    <span class="n">trainset</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">trainset</span><span class="p">()</span> <span class="c1"># ËÆ≠ÁªÉÈõÜ</span>
    <span class="n">maskset</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">maskset</span><span class="p">()</span> <span class="c1"># mask Êï∞ÊçÆÈõÜ</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">maskset</span><span class="p">):</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask_op</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
        <span class="n">total_steps</span> <span class="o">+=</span> <span class="n">model</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="n">epoch_iter</span> <span class="o">+=</span> <span class="n">model</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="c1"># it not only sets the input data with mask,</span>
        <span class="c1">#  but also sets the latent mask.</span>
        <span class="n">model</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">set_gt_latent</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">optimize_parameters</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">total_steps</span> <span class="o">%</span> <span class="n">display_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">real_A</span><span class="p">,</span> <span class="n">real_B</span><span class="p">,</span> <span class="n">fake_B</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_current_visuals</span><span class="p">()</span>
            <span class="c1"># real_A=input, real_B=ground truth fake_b=output</span>
            <span class="n">pic</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">real_A</span><span class="p">,</span> <span class="n">real_B</span><span class="p">,</span> <span class="n">fake_B</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>
            <span class="n">image_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;epoch</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">total_steps</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">.png&quot;</span>
            <span class="n">save_image</span><span class="p">(</span><span class="n">pic</span><span class="p">,</span> <span class="n">image_save_dir</span><span class="o">/</span><span class="n">image_name</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">total_steps</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">errors</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_current_errors</span><span class="p">()</span>
            <span class="n">t</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">iter_start_time</span><span class="p">)</span> <span class="o">/</span> <span class="n">model</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Epoch/total_steps/alpha-beta: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">total_steps</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">save_epoch_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;‰øùÂ≠òÊ®°Âûã Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">, iters </span><span class="si">{</span><span class="n">total_steps</span><span class="si">}</span><span class="s1"> Âú® </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">save_dir</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s1">&#39;Epoch/Epochs </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">epochs</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s1"> Ëä±Ë¥πÊó∂Èó¥Ôºö</span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start_time</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">update_learning_rate</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch/total_steps/alpha-beta: 0/100/1-1 {&#39;G_GAN&#39;: 5.674766540527344, &#39;G_L1&#39;: 82.69174194335938, &#39;D&#39;: 0.8737938404083252, &#39;F&#39;: 0.11914195865392685}
Epoch/total_steps/alpha-beta: 0/200/1-1 {&#39;G_GAN&#39;: 6.374797821044922, &#39;G_L1&#39;: 94.81090545654297, &#39;D&#39;: 0.49556198716163635, &#39;F&#39;: 0.0632593184709549}
Epoch/total_steps/alpha-beta: 0/300/1-1 {&#39;G_GAN&#39;: 5.32965087890625, &#39;G_L1&#39;: 53.254173278808594, &#39;D&#39;: 0.8827108144760132, &#39;F&#39;: 0.036601416766643524}
Epoch/total_steps/alpha-beta: 0/400/1-1 {&#39;G_GAN&#39;: 5.552875995635986, &#39;G_L1&#39;: 72.80084228515625, &#39;D&#39;: 0.9750968217849731, &#39;F&#39;: 0.1065833568572998}
Epoch/total_steps/alpha-beta: 0/500/1-1 {&#39;G_GAN&#39;: 6.736172676086426, &#39;G_L1&#39;: 52.51408386230469, &#39;D&#39;: 0.4093303084373474, &#39;F&#39;: 0.025163184851408005}
Epoch/total_steps/alpha-beta: 0/600/1-1 {&#39;G_GAN&#39;: 7.541547775268555, &#39;G_L1&#39;: 112.79335021972656, &#39;D&#39;: 0.12005992233753204, &#39;F&#39;: 0.01236003264784813}
Epoch/total_steps/alpha-beta: 0/700/1-1 {&#39;G_GAN&#39;: 6.069731712341309, &#39;G_L1&#39;: 47.136253356933594, &#39;D&#39;: 0.7534974813461304, &#39;F&#39;: 0.020309582352638245}
Epoch/total_steps/alpha-beta: 0/800/1-1 {&#39;G_GAN&#39;: 5.450955390930176, &#39;G_L1&#39;: 65.66997528076172, &#39;D&#39;: 0.8705061674118042, &#39;F&#39;: 0.060515161603689194}
Epoch/total_steps/alpha-beta: 0/900/1-1 {&#39;G_GAN&#39;: 6.46990966796875, &#39;G_L1&#39;: 75.23210144042969, &#39;D&#39;: 0.5232571363449097, &#39;F&#39;: 0.013653963804244995}
Epoch/total_steps/alpha-beta: 0/1000/1-1 {&#39;G_GAN&#39;: 6.787680625915527, &#39;G_L1&#39;: 50.51403045654297, &#39;D&#39;: 0.5192381143569946, &#39;F&#39;: 0.04075857624411583}
Epoch/total_steps/alpha-beta: 0/1100/1-1 {&#39;G_GAN&#39;: 6.793991565704346, &#39;G_L1&#39;: 49.92967224121094, &#39;D&#39;: 0.2963847517967224, &#39;F&#39;: 0.0138858025893569}
Epoch/total_steps/alpha-beta: 0/1200/1-1 {&#39;G_GAN&#39;: 6.908201694488525, &#39;G_L1&#39;: 46.24758529663086, &#39;D&#39;: 0.4864062964916229, &#39;F&#39;: 0.04695504158735275}
Epoch/total_steps/alpha-beta: 0/1300/1-1 {&#39;G_GAN&#39;: 7.270747184753418, &#39;G_L1&#39;: 39.49330520629883, &#39;D&#39;: 0.31095367670059204, &#39;F&#39;: 0.01762988604605198}
Epoch/total_steps/alpha-beta: 0/1400/1-1 {&#39;G_GAN&#39;: 7.141920566558838, &#39;G_L1&#39;: 90.7420883178711, &#39;D&#39;: 0.6246428489685059, &#39;F&#39;: 0.01644831895828247}
Epoch/total_steps/alpha-beta: 0/1500/1-1 {&#39;G_GAN&#39;: 7.252217769622803, &#39;G_L1&#39;: 80.00102233886719, &#39;D&#39;: 0.35810133814811707, &#39;F&#39;: 0.009534837678074837}
Epoch/total_steps/alpha-beta: 0/1600/1-1 {&#39;G_GAN&#39;: 6.656037330627441, &#39;G_L1&#39;: 78.57904815673828, &#39;D&#39;: 0.2720087170600891, &#39;F&#39;: 0.07613863795995712}
Epoch/total_steps/alpha-beta: 0/1700/1-1 {&#39;G_GAN&#39;: 6.747368812561035, &#39;G_L1&#39;: 53.054107666015625, &#39;D&#39;: 0.3349810242652893, &#39;F&#39;: 0.014762762933969498}
Epoch/total_steps/alpha-beta: 0/1800/1-1 {&#39;G_GAN&#39;: 7.704248428344727, &#39;G_L1&#39;: 49.09436798095703, &#39;D&#39;: 0.4437968134880066, &#39;F&#39;: 0.00922537874430418}
Epoch/total_steps/alpha-beta: 0/1900/1-1 {&#39;G_GAN&#39;: 7.979488372802734, &#39;G_L1&#39;: 38.44279479980469, &#39;D&#39;: 0.04135489463806152, &#39;F&#39;: 0.018109498545527458}
Epoch/total_steps/alpha-beta: 0/2000/1-1 {&#39;G_GAN&#39;: 8.089262962341309, &#39;G_L1&#39;: 29.569683074951172, &#39;D&#39;: 0.4374288320541382, &#39;F&#39;: 0.017628138884902}
Epoch/total_steps/alpha-beta: 0/2100/1-1 {&#39;G_GAN&#39;: 7.554396152496338, &#39;G_L1&#39;: 56.426353454589844, &#39;D&#39;: 0.5255504846572876, &#39;F&#39;: 0.012429237365722656}
Epoch/total_steps/alpha-beta: 0/2200/1-1 {&#39;G_GAN&#39;: 6.848667144775391, &#39;G_L1&#39;: 77.50362396240234, &#39;D&#39;: 0.3149612545967102, &#39;F&#39;: 0.01767728291451931}
Epoch/total_steps/alpha-beta: 0/2300/1-1 {&#39;G_GAN&#39;: 7.116144180297852, &#39;G_L1&#39;: 66.12065124511719, &#39;D&#39;: 0.15158146619796753, &#39;F&#39;: 0.01847725175321102}
Epoch/total_steps/alpha-beta: 0/2400/1-1 {&#39;G_GAN&#39;: 7.701614856719971, &#39;G_L1&#39;: 50.05036544799805, &#39;D&#39;: 0.35271155834198, &#39;F&#39;: 0.013561561703681946}
Epoch/total_steps/alpha-beta: 0/2500/1-1 {&#39;G_GAN&#39;: 8.142476081848145, &#39;G_L1&#39;: 56.733131408691406, &#39;D&#39;: 0.09474793076515198, &#39;F&#39;: 0.029223527759313583}
Epoch/total_steps/alpha-beta: 0/2600/1-1 {&#39;G_GAN&#39;: 7.383360862731934, &#39;G_L1&#39;: 66.50104522705078, &#39;D&#39;: 0.17739753425121307, &#39;F&#39;: 0.008837783709168434}
Epoch/total_steps/alpha-beta: 0/2700/1-1 {&#39;G_GAN&#39;: 8.906221389770508, &#39;G_L1&#39;: 34.362144470214844, &#39;D&#39;: 0.382310688495636, &#39;F&#39;: 0.016032561659812927}
Epoch/total_steps/alpha-beta: 0/2800/1-1 {&#39;G_GAN&#39;: 6.245453357696533, &#39;G_L1&#39;: 51.14915466308594, &#39;D&#39;: 0.5112225413322449, &#39;F&#39;: 0.012845803052186966}
Epoch/total_steps/alpha-beta: 0/2900/1-1 {&#39;G_GAN&#39;: 7.2987284660339355, &#39;G_L1&#39;: 58.71614074707031, &#39;D&#39;: 0.3012716770172119, &#39;F&#39;: 0.006878486834466457}
Epoch/total_steps/alpha-beta: 0/3000/1-1 {&#39;G_GAN&#39;: 5.918247222900391, &#39;G_L1&#39;: 33.839569091796875, &#39;D&#39;: 0.47583073377609253, &#39;F&#39;: 0.023110026493668556}
Epoch/total_steps/alpha-beta: 0/3100/1-1 {&#39;G_GAN&#39;: 7.544097900390625, &#39;G_L1&#39;: 96.7215805053711, &#39;D&#39;: 0.08623941242694855, &#39;F&#39;: 0.010666212067008018}
Epoch/total_steps/alpha-beta: 0/3200/1-1 {&#39;G_GAN&#39;: 5.972902297973633, &#39;G_L1&#39;: 45.39277648925781, &#39;D&#39;: 0.6999977231025696, &#39;F&#39;: 0.013009447604417801}
Epoch/total_steps/alpha-beta: 0/3300/1-1 {&#39;G_GAN&#39;: 7.454391002655029, &#39;G_L1&#39;: 42.21316146850586, &#39;D&#39;: 0.3786676228046417, &#39;F&#39;: 0.023663945496082306}
Epoch/total_steps/alpha-beta: 0/3400/1-1 {&#39;G_GAN&#39;: 8.168795585632324, &#39;G_L1&#39;: 50.32633972167969, &#39;D&#39;: 0.06947940587997437, &#39;F&#39;: 0.0052123898640275}
Epoch/total_steps/alpha-beta: 0/3500/1-1 {&#39;G_GAN&#39;: 7.665689468383789, &#39;G_L1&#39;: 47.38014221191406, &#39;D&#39;: 0.11292305588722229, &#39;F&#39;: 0.007362126372754574}
Epoch/total_steps/alpha-beta: 0/3600/1-1 {&#39;G_GAN&#39;: 8.304533004760742, &#39;G_L1&#39;: 64.66524505615234, &#39;D&#39;: 0.07515833526849747, &#39;F&#39;: 0.008023531176149845}
Epoch/total_steps/alpha-beta: 0/3700/1-1 {&#39;G_GAN&#39;: 8.682092666625977, &#39;G_L1&#39;: 39.63965606689453, &#39;D&#39;: 0.25657469034194946, &#39;F&#39;: 0.014217937365174294}
Epoch/total_steps/alpha-beta: 0/3800/1-1 {&#39;G_GAN&#39;: 7.638301849365234, &#39;G_L1&#39;: 78.36988067626953, &#39;D&#39;: 0.09640045464038849, &#39;F&#39;: 0.014670801348984241}
Epoch/total_steps/alpha-beta: 0/3900/1-1 {&#39;G_GAN&#39;: 6.8892364501953125, &#39;G_L1&#39;: 48.8631591796875, &#39;D&#39;: 0.5110245943069458, &#39;F&#39;: 0.0077681331895291805}
Epoch/total_steps/alpha-beta: 0/4000/1-1 {&#39;G_GAN&#39;: 7.771721363067627, &#39;G_L1&#39;: 56.03171157836914, &#39;D&#39;: 0.04583849757909775, &#39;F&#39;: 0.008565093390643597}
Epoch/total_steps/alpha-beta: 0/4100/1-1 {&#39;G_GAN&#39;: 7.996473789215088, &#39;G_L1&#39;: 56.72932815551758, &#39;D&#39;: 0.13362935185432434, &#39;F&#39;: 0.011361945420503616}
Epoch/total_steps/alpha-beta: 0/4200/1-1 {&#39;G_GAN&#39;: 7.996485233306885, &#39;G_L1&#39;: 42.705421447753906, &#39;D&#39;: 0.04230982065200806, &#39;F&#39;: 0.016090290620923042}
Epoch/total_steps/alpha-beta: 0/4300/1-1 {&#39;G_GAN&#39;: 6.5630083084106445, &#39;G_L1&#39;: 77.21231079101562, &#39;D&#39;: 0.31008175015449524, &#39;F&#39;: 0.007575363852083683}
Epoch/total_steps/alpha-beta: 0/4400/1-1 {&#39;G_GAN&#39;: 7.481277942657471, &#39;G_L1&#39;: 57.51869583129883, &#39;D&#39;: 0.22858652472496033, &#39;F&#39;: 0.008757248520851135}
Epoch/total_steps/alpha-beta: 0/4500/1-1 {&#39;G_GAN&#39;: 8.45147705078125, &#39;G_L1&#39;: 37.629486083984375, &#39;D&#39;: 0.12803904712200165, &#39;F&#39;: 0.009547237306833267}
Epoch/total_steps/alpha-beta: 0/4600/1-1 {&#39;G_GAN&#39;: 8.402596473693848, &#39;G_L1&#39;: 63.712711334228516, &#39;D&#39;: 0.12649144232273102, &#39;F&#39;: 0.012351367622613907}
Epoch/total_steps/alpha-beta: 0/4700/1-1 {&#39;G_GAN&#39;: 7.6600847244262695, &#39;G_L1&#39;: 49.206878662109375, &#39;D&#39;: 0.06945803016424179, &#39;F&#39;: 0.008419311605393887}
Epoch/total_steps/alpha-beta: 0/4800/1-1 {&#39;G_GAN&#39;: 7.621247291564941, &#39;G_L1&#39;: 62.317752838134766, &#39;D&#39;: 0.392180860042572, &#39;F&#39;: 0.008641136810183525}
Epoch/total_steps/alpha-beta: 0/4900/1-1 {&#39;G_GAN&#39;: 6.153536796569824, &#39;G_L1&#39;: 73.89155578613281, &#39;D&#39;: 1.9577667713165283, &#39;F&#39;: 0.004571779165416956}
Epoch/total_steps/alpha-beta: 0/5000/1-1 {&#39;G_GAN&#39;: 7.6374430656433105, &#39;G_L1&#39;: 42.614261627197266, &#39;D&#39;: 0.15651223063468933, &#39;F&#39;: 0.007243161089718342}
Epoch/total_steps/alpha-beta: 0/5100/1-1 {&#39;G_GAN&#39;: 6.886175632476807, &#39;G_L1&#39;: 56.85025405883789, &#39;D&#39;: 0.23289775848388672, &#39;F&#39;: 0.005355427041649818}
Epoch/total_steps/alpha-beta: 0/5200/1-1 {&#39;G_GAN&#39;: 5.798615455627441, &#39;G_L1&#39;: 39.678470611572266, &#39;D&#39;: 1.16788649559021, &#39;F&#39;: 0.010754863731563091}
Epoch/total_steps/alpha-beta: 0/5300/1-1 {&#39;G_GAN&#39;: 8.263847351074219, &#39;G_L1&#39;: 86.83111572265625, &#39;D&#39;: 0.10561434924602509, &#39;F&#39;: 0.006744322367012501}
Epoch/total_steps/alpha-beta: 0/5400/1-1 {&#39;G_GAN&#39;: 6.803694725036621, &#39;G_L1&#39;: 70.46418762207031, &#39;D&#39;: 0.3951355218887329, &#39;F&#39;: 0.005242636427283287}
Epoch/total_steps/alpha-beta: 0/5500/1-1 {&#39;G_GAN&#39;: 7.981578826904297, &#39;G_L1&#39;: 108.26988220214844, &#39;D&#39;: 0.06490228325128555, &#39;F&#39;: 0.013407368212938309}
Epoch/total_steps/alpha-beta: 0/5600/1-1 {&#39;G_GAN&#39;: 7.320289134979248, &#39;G_L1&#39;: 52.941715240478516, &#39;D&#39;: 0.1379150152206421, &#39;F&#39;: 0.0030706580728292465}
Epoch/total_steps/alpha-beta: 0/5700/1-1 {&#39;G_GAN&#39;: 7.973141193389893, &#39;G_L1&#39;: 59.55439376831055, &#39;D&#39;: 0.030175207182765007, &#39;F&#39;: 0.01176734920591116}
Epoch/total_steps/alpha-beta: 0/5800/1-1 {&#39;G_GAN&#39;: 8.284833908081055, &#39;G_L1&#39;: 60.21121597290039, &#39;D&#39;: 0.06262273341417313, &#39;F&#39;: 0.006977483630180359}
Epoch/total_steps/alpha-beta: 0/5900/1-1 {&#39;G_GAN&#39;: 8.258224487304688, &#39;G_L1&#39;: 35.075706481933594, &#39;D&#39;: 0.04807504266500473, &#39;F&#39;: 0.006213567219674587}
Epoch/total_steps/alpha-beta: 0/6000/1-1 {&#39;G_GAN&#39;: 8.383806228637695, &#39;G_L1&#39;: 46.3287353515625, &#39;D&#39;: 0.08396550267934799, &#39;F&#39;: 0.0107817891985178}
Epoch/total_steps/alpha-beta: 0/6100/1-1 {&#39;G_GAN&#39;: 8.344456672668457, &#39;G_L1&#39;: 59.95425033569336, &#39;D&#39;: 0.07266698777675629, &#39;F&#39;: 0.008597245439887047}
Epoch/total_steps/alpha-beta: 0/6200/1-1 {&#39;G_GAN&#39;: 6.1311469078063965, &#39;G_L1&#39;: 48.754188537597656, &#39;D&#39;: 0.5149170756340027, &#39;F&#39;: 0.004501000978052616}
Epoch/total_steps/alpha-beta: 0/6300/1-1 {&#39;G_GAN&#39;: 7.149686813354492, &#39;G_L1&#39;: 71.15396118164062, &#39;D&#39;: 0.2500658631324768, &#39;F&#39;: 0.007312756031751633}
Epoch/total_steps/alpha-beta: 0/6400/1-1 {&#39;G_GAN&#39;: 7.40749454498291, &#39;G_L1&#39;: 42.18881607055664, &#39;D&#39;: 0.3081147372722626, &#39;F&#39;: 0.004056393634527922}
Epoch/total_steps/alpha-beta: 0/6500/1-1 {&#39;G_GAN&#39;: 6.996418476104736, &#39;G_L1&#39;: 30.674816131591797, &#39;D&#39;: 0.25620943307876587, &#39;F&#39;: 0.015434334985911846}
Epoch/total_steps/alpha-beta: 0/6600/1-1 {&#39;G_GAN&#39;: 8.121455192565918, &#39;G_L1&#39;: 49.59626770019531, &#39;D&#39;: 0.028358401730656624, &#39;F&#39;: 0.006339492276310921}
Epoch/total_steps/alpha-beta: 0/6700/1-1 {&#39;G_GAN&#39;: 8.35389518737793, &#39;G_L1&#39;: 45.81657409667969, &#39;D&#39;: 0.12101450562477112, &#39;F&#39;: 0.0031661679968237877}
Epoch/total_steps/alpha-beta: 0/6800/1-1 {&#39;G_GAN&#39;: 7.042941093444824, &#39;G_L1&#39;: 40.504085540771484, &#39;D&#39;: 0.1503693163394928, &#39;F&#39;: 0.002462363336235285}
Epoch/total_steps/alpha-beta: 0/6900/1-1 {&#39;G_GAN&#39;: 7.426477909088135, &#39;G_L1&#39;: 55.55573654174805, &#39;D&#39;: 0.05519334226846695, &#39;F&#39;: 0.007539939135313034}
Epoch/total_steps/alpha-beta: 0/7000/1-1 {&#39;G_GAN&#39;: 7.626007080078125, &#39;G_L1&#39;: 70.24970245361328, &#39;D&#39;: 0.08646371215581894, &#39;F&#39;: 0.0026434799656271935}
Epoch/total_steps/alpha-beta: 0/7100/1-1 {&#39;G_GAN&#39;: 7.605014324188232, &#39;G_L1&#39;: 41.01420974731445, &#39;D&#39;: 0.08497186005115509, &#39;F&#39;: 0.00252027646638453}
Epoch/total_steps/alpha-beta: 0/7200/1-1 {&#39;G_GAN&#39;: 6.084473609924316, &#39;G_L1&#39;: 32.2652587890625, &#39;D&#39;: 0.4734061658382416, &#39;F&#39;: 0.0019388592336326838}
Epoch/total_steps/alpha-beta: 0/7300/1-1 {&#39;G_GAN&#39;: 7.018969535827637, &#39;G_L1&#39;: 85.42520904541016, &#39;D&#39;: 0.27489325404167175, &#39;F&#39;: 0.0023627576883882284}
Epoch/total_steps/alpha-beta: 0/7400/1-1 {&#39;G_GAN&#39;: 6.402486801147461, &#39;G_L1&#39;: 42.54283142089844, &#39;D&#39;: 0.27588963508605957, &#39;F&#39;: 0.004182430449873209}
Epoch/total_steps/alpha-beta: 0/7500/1-1 {&#39;G_GAN&#39;: 5.938089370727539, &#39;G_L1&#39;: 31.036848068237305, &#39;D&#39;: 0.7004063129425049, &#39;F&#39;: 0.002694114111363888}
Epoch/total_steps/alpha-beta: 0/7600/1-1 {&#39;G_GAN&#39;: 8.16080379486084, &#39;G_L1&#39;: 52.37434005737305, &#39;D&#39;: 0.05677836015820503, &#39;F&#39;: 0.0037008144427090883}
Epoch/total_steps/alpha-beta: 0/7700/1-1 {&#39;G_GAN&#39;: 7.534536838531494, &#39;G_L1&#39;: 48.51013946533203, &#39;D&#39;: 0.33154428005218506, &#39;F&#39;: 0.0018072292441502213}
Epoch/total_steps/alpha-beta: 0/7800/1-1 {&#39;G_GAN&#39;: 7.943121910095215, &#39;G_L1&#39;: 62.80522155761719, &#39;D&#39;: 0.0310518816113472, &#39;F&#39;: 0.0057301889173686504}
Epoch/total_steps/alpha-beta: 0/7900/1-1 {&#39;G_GAN&#39;: 8.19321060180664, &#39;G_L1&#39;: 34.802223205566406, &#39;D&#39;: 0.04974427446722984, &#39;F&#39;: 0.0026190797798335552}
Epoch/total_steps/alpha-beta: 0/8000/1-1 {&#39;G_GAN&#39;: 7.1175432205200195, &#39;G_L1&#39;: 38.05084991455078, &#39;D&#39;: 0.21175993978977203, &#39;F&#39;: 0.004107722546905279}
Epoch/total_steps/alpha-beta: 0/8100/1-1 {&#39;G_GAN&#39;: 7.780276298522949, &#39;G_L1&#39;: 74.96903228759766, &#39;D&#39;: 0.03703831136226654, &#39;F&#39;: 0.010518229566514492}
Epoch/total_steps/alpha-beta: 0/8200/1-1 {&#39;G_GAN&#39;: 7.708995819091797, &#39;G_L1&#39;: 33.80449295043945, &#39;D&#39;: 0.030776165425777435, &#39;F&#39;: 0.009684702381491661}
Epoch/total_steps/alpha-beta: 0/8300/1-1 {&#39;G_GAN&#39;: 7.522763252258301, &#39;G_L1&#39;: 23.80935287475586, &#39;D&#39;: 0.049943707883358, &#39;F&#39;: 0.0021161306649446487}
Epoch/total_steps/alpha-beta: 0/8400/1-1 {&#39;G_GAN&#39;: 7.257826805114746, &#39;G_L1&#39;: 29.893531799316406, &#39;D&#39;: 0.14615777134895325, &#39;F&#39;: 0.0021438393741846085}
Epoch/total_steps/alpha-beta: 0/8500/1-1 {&#39;G_GAN&#39;: 8.02918815612793, &#39;G_L1&#39;: 61.064064025878906, &#39;D&#39;: 0.13911926746368408, &#39;F&#39;: 0.004984264727681875}
Epoch/total_steps/alpha-beta: 0/8600/1-1 {&#39;G_GAN&#39;: 7.9883952140808105, &#39;G_L1&#39;: 38.462806701660156, &#39;D&#39;: 0.03824824094772339, &#39;F&#39;: 0.009075062349438667}
Epoch/total_steps/alpha-beta: 0/8700/1-1 {&#39;G_GAN&#39;: 6.465782165527344, &#39;G_L1&#39;: 81.16838073730469, &#39;D&#39;: 0.3243834376335144, &#39;F&#39;: 0.0027307465206831694}
Epoch/total_steps/alpha-beta: 0/8800/1-1 {&#39;G_GAN&#39;: 7.645304203033447, &#39;G_L1&#39;: 40.95176696777344, &#39;D&#39;: 0.0751340240240097, &#39;F&#39;: 0.00923872459679842}
Epoch/total_steps/alpha-beta: 0/8900/1-1 {&#39;G_GAN&#39;: 8.011362075805664, &#39;G_L1&#39;: 33.08267593383789, &#39;D&#39;: 0.03689172863960266, &#39;F&#39;: 0.0031728052999824286}
Epoch/total_steps/alpha-beta: 0/9000/1-1 {&#39;G_GAN&#39;: 7.774505615234375, &#39;G_L1&#39;: 50.78799819946289, &#39;D&#39;: 0.07414526492357254, &#39;F&#39;: 0.0015776701038703322}
Epoch/total_steps/alpha-beta: 0/9100/1-1 {&#39;G_GAN&#39;: 7.417238235473633, &#39;G_L1&#39;: 43.68589401245117, &#39;D&#39;: 0.36115360260009766, &#39;F&#39;: 0.002895291429013014}
Epoch/total_steps/alpha-beta: 0/9200/1-1 {&#39;G_GAN&#39;: 7.5521321296691895, &#39;G_L1&#39;: 43.371826171875, &#39;D&#39;: 0.11037002503871918, &#39;F&#39;: 0.0032599885016679764}
Epoch/total_steps/alpha-beta: 0/9300/1-1 {&#39;G_GAN&#39;: 7.891937255859375, &#39;G_L1&#39;: 63.42267990112305, &#39;D&#39;: 0.24139544367790222, &#39;F&#39;: 0.0019102185033261776}
Epoch/total_steps/alpha-beta: 0/9400/1-1 {&#39;G_GAN&#39;: 7.2336506843566895, &#39;G_L1&#39;: 56.6075553894043, &#39;D&#39;: 0.24763208627700806, &#39;F&#39;: 0.003271276131272316}
Epoch/total_steps/alpha-beta: 0/9500/1-1 {&#39;G_GAN&#39;: 9.585886001586914, &#39;G_L1&#39;: 60.436092376708984, &#39;D&#39;: 1.226494312286377, &#39;F&#39;: 0.0016162550309672952}
Epoch/total_steps/alpha-beta: 0/9600/1-1 {&#39;G_GAN&#39;: 6.00008487701416, &#39;G_L1&#39;: 38.142555236816406, &#39;D&#39;: 0.5073889493942261, &#39;F&#39;: 0.0017748342361301184}
Epoch/total_steps/alpha-beta: 0/9700/1-1 {&#39;G_GAN&#39;: 7.636424541473389, &#39;G_L1&#39;: 46.69641876220703, &#39;D&#39;: 0.13829797506332397, &#39;F&#39;: 0.0016540708020329475}
Epoch/total_steps/alpha-beta: 0/9800/1-1 {&#39;G_GAN&#39;: 7.472754001617432, &#39;G_L1&#39;: 50.239501953125, &#39;D&#39;: 0.06921680271625519, &#39;F&#39;: 0.002527147065848112}
Epoch/total_steps/alpha-beta: 0/9900/1-1 {&#39;G_GAN&#39;: 8.941873550415039, &#39;G_L1&#39;: 67.71498107910156, &#39;D&#39;: 0.16962285339832306, &#39;F&#39;: 0.0020194686949253082}
Epoch/total_steps/alpha-beta: 0/10000/1-1 {&#39;G_GAN&#39;: 8.186840057373047, &#39;G_L1&#39;: 68.31453704833984, &#39;D&#39;: 0.6574151515960693, &#39;F&#39;: 0.004415786825120449}
Epoch/total_steps/alpha-beta: 0/10100/1-1 {&#39;G_GAN&#39;: 6.864284515380859, &#39;G_L1&#39;: 66.50794219970703, &#39;D&#39;: 0.22174230217933655, &#39;F&#39;: 0.01656476967036724}
Epoch/total_steps/alpha-beta: 0/10200/1-1 {&#39;G_GAN&#39;: 7.495953559875488, &#39;G_L1&#39;: 36.597225189208984, &#39;D&#39;: 0.06352269649505615, &#39;F&#39;: 0.01595478691160679}
Epoch/total_steps/alpha-beta: 0/10300/1-1 {&#39;G_GAN&#39;: 8.345966339111328, &#39;G_L1&#39;: 77.30419158935547, &#39;D&#39;: 0.08971921354532242, &#39;F&#39;: 0.003577121999114752}
Epoch/total_steps/alpha-beta: 0/10400/1-1 {&#39;G_GAN&#39;: 8.669347763061523, &#39;G_L1&#39;: 30.070919036865234, &#39;D&#39;: 0.9127235412597656, &#39;F&#39;: 0.002194229979068041}
Epoch/total_steps/alpha-beta: 0/10500/1-1 {&#39;G_GAN&#39;: 8.475579261779785, &#39;G_L1&#39;: 80.79313659667969, &#39;D&#39;: 0.05295795202255249, &#39;F&#39;: 0.0015146638033911586}
Epoch/total_steps/alpha-beta: 0/10600/1-1 {&#39;G_GAN&#39;: 8.784976959228516, &#39;G_L1&#39;: 73.26616668701172, &#39;D&#39;: 0.08933505415916443, &#39;F&#39;: 0.0032597589306533337}
Epoch/total_steps/alpha-beta: 0/10700/1-1 {&#39;G_GAN&#39;: 6.787628650665283, &#39;G_L1&#39;: 38.454734802246094, &#39;D&#39;: 0.3249225616455078, &#39;F&#39;: 0.0034505603834986687}
Epoch/total_steps/alpha-beta: 0/10800/1-1 {&#39;G_GAN&#39;: 8.730010032653809, &#39;G_L1&#39;: 60.60959243774414, &#39;D&#39;: 0.30015599727630615, &#39;F&#39;: 0.0032948614098131657}
Epoch/total_steps/alpha-beta: 0/10900/1-1 {&#39;G_GAN&#39;: 7.004982948303223, &#39;G_L1&#39;: 32.01048278808594, &#39;D&#39;: 0.19470490515232086, &#39;F&#39;: 0.0031665561255067587}
Epoch/total_steps/alpha-beta: 0/11000/1-1 {&#39;G_GAN&#39;: 9.061315536499023, &#39;G_L1&#39;: 68.95564270019531, &#39;D&#39;: 0.19244731962680817, &#39;F&#39;: 0.0034508907701820135}
Epoch/total_steps/alpha-beta: 0/11100/1-1 {&#39;G_GAN&#39;: 8.578391075134277, &#39;G_L1&#39;: 82.3022689819336, &#39;D&#39;: 0.11175152659416199, &#39;F&#39;: 0.003092898055911064}
Epoch/total_steps/alpha-beta: 0/11200/1-1 {&#39;G_GAN&#39;: 8.29755973815918, &#39;G_L1&#39;: 59.436710357666016, &#39;D&#39;: 0.05363302677869797, &#39;F&#39;: 0.00201236829161644}
Epoch/total_steps/alpha-beta: 0/11300/1-1 {&#39;G_GAN&#39;: 7.944901466369629, &#39;G_L1&#39;: 54.53965759277344, &#39;D&#39;: 0.040874823927879333, &#39;F&#39;: 0.01685798540711403}
Epoch/total_steps/alpha-beta: 0/11400/1-1 {&#39;G_GAN&#39;: 6.979119777679443, &#39;G_L1&#39;: 53.772342681884766, &#39;D&#39;: 0.16557006537914276, &#39;F&#39;: 0.004237089306116104}
Epoch/total_steps/alpha-beta: 0/11500/1-1 {&#39;G_GAN&#39;: 8.27999496459961, &#39;G_L1&#39;: 67.8206558227539, &#39;D&#39;: 0.17752400040626526, &#39;F&#39;: 0.018265556544065475}
Epoch/total_steps/alpha-beta: 0/11600/1-1 {&#39;G_GAN&#39;: 8.279304504394531, &#39;G_L1&#39;: 65.64483642578125, &#39;D&#39;: 0.08764566481113434, &#39;F&#39;: 0.005941405892372131}
Epoch/total_steps/alpha-beta: 0/11700/1-1 {&#39;G_GAN&#39;: 8.147682189941406, &#39;G_L1&#39;: 83.97203063964844, &#39;D&#39;: 0.05911638215184212, &#39;F&#39;: 0.0014920537360012531}
Epoch/total_steps/alpha-beta: 0/11800/1-1 {&#39;G_GAN&#39;: 7.444559097290039, &#39;G_L1&#39;: 51.360069274902344, &#39;D&#39;: 0.08621696382761002, &#39;F&#39;: 0.0015211268328130245}
Epoch/total_steps/alpha-beta: 0/11900/1-1 {&#39;G_GAN&#39;: 6.389288425445557, &#39;G_L1&#39;: 64.01058197021484, &#39;D&#39;: 0.4870639443397522, &#39;F&#39;: 0.004923909902572632}
Epoch/total_steps/alpha-beta: 0/12000/1-1 {&#39;G_GAN&#39;: 7.441242694854736, &#39;G_L1&#39;: 101.56706237792969, &#39;D&#39;: 0.19994515180587769, &#39;F&#39;: 0.004831552039831877}
‰øùÂ≠òÊ®°Âûã Epoch 0, iters 12000 Âú® D:\BaiduNetdiskWorkspace\result\CSA-512-face
Epoch/Epochs 0/499 Ëä±Ë¥πÊó∂Èó¥Ôºö210050.6013519764s
learning rate = 0.0002
Epoch/total_steps/alpha-beta: 1/12100/1-1 {&#39;G_GAN&#39;: 6.965084552764893, &#39;G_L1&#39;: 32.1517333984375, &#39;D&#39;: 0.23298028111457825, &#39;F&#39;: 0.001660623587667942}
Epoch/total_steps/alpha-beta: 1/12200/1-1 {&#39;G_GAN&#39;: 8.568268775939941, &#39;G_L1&#39;: 49.834041595458984, &#39;D&#39;: 0.11470457911491394, &#39;F&#39;: 0.003575363429263234}
Epoch/total_steps/alpha-beta: 1/12300/1-1 {&#39;G_GAN&#39;: 6.744634628295898, &#39;G_L1&#39;: 71.9154052734375, &#39;D&#39;: 0.26927173137664795, &#39;F&#39;: 0.003682998474687338}
Epoch/total_steps/alpha-beta: 1/12400/1-1 {&#39;G_GAN&#39;: 6.9594926834106445, &#39;G_L1&#39;: 56.10301971435547, &#39;D&#39;: 0.18023376166820526, &#39;F&#39;: 0.0034145619720220566}
Epoch/total_steps/alpha-beta: 1/12500/1-1 {&#39;G_GAN&#39;: 8.631425857543945, &#39;G_L1&#39;: 39.09158706665039, &#39;D&#39;: 0.36337924003601074, &#39;F&#39;: 0.001938710454851389}
Epoch/total_steps/alpha-beta: 1/12600/1-1 {&#39;G_GAN&#39;: 6.918540000915527, &#39;G_L1&#39;: 33.414371490478516, &#39;D&#39;: 0.27908551692962646, &#39;F&#39;: 0.004072829149663448}
Epoch/total_steps/alpha-beta: 1/12700/1-1 {&#39;G_GAN&#39;: 7.9742631912231445, &#39;G_L1&#39;: 39.070735931396484, &#39;D&#39;: 0.06639857590198517, &#39;F&#39;: 0.001771403243765235}
Epoch/total_steps/alpha-beta: 1/12800/1-1 {&#39;G_GAN&#39;: 7.309837341308594, &#39;G_L1&#39;: 72.9410171508789, &#39;D&#39;: 0.10785399377346039, &#39;F&#39;: 0.019198037683963776}
Epoch/total_steps/alpha-beta: 1/12900/1-1 {&#39;G_GAN&#39;: 6.211674213409424, &#39;G_L1&#39;: 33.85203552246094, &#39;D&#39;: 0.5952372550964355, &#39;F&#39;: 0.003889149986207485}
Epoch/total_steps/alpha-beta: 1/13000/1-1 {&#39;G_GAN&#39;: 7.215796947479248, &#39;G_L1&#39;: 36.57707595825195, &#39;D&#39;: 0.14488071203231812, &#39;F&#39;: 0.003104927483946085}
Epoch/total_steps/alpha-beta: 1/13100/1-1 {&#39;G_GAN&#39;: 7.988907814025879, &#39;G_L1&#39;: 44.266693115234375, &#39;D&#39;: 0.024571159854531288, &#39;F&#39;: 0.001009827246889472}
Epoch/total_steps/alpha-beta: 1/13200/1-1 {&#39;G_GAN&#39;: 6.033517837524414, &#39;G_L1&#39;: 42.82467269897461, &#39;D&#39;: 0.7101396322250366, &#39;F&#39;: 0.013241153210401535}
Epoch/total_steps/alpha-beta: 1/13300/1-1 {&#39;G_GAN&#39;: 6.72196626663208, &#39;G_L1&#39;: 71.67842864990234, &#39;D&#39;: 0.2942380905151367, &#39;F&#39;: 0.002327448222786188}
Epoch/total_steps/alpha-beta: 1/13400/1-1 {&#39;G_GAN&#39;: 5.743076324462891, &#39;G_L1&#39;: 46.365989685058594, &#39;D&#39;: 0.5834101438522339, &#39;F&#39;: 0.001597607508301735}
Epoch/total_steps/alpha-beta: 1/13500/1-1 {&#39;G_GAN&#39;: 8.59292984008789, &#39;G_L1&#39;: 64.403564453125, &#39;D&#39;: 0.10985682904720306, &#39;F&#39;: 0.01624016836285591}
Epoch/total_steps/alpha-beta: 1/13600/1-1 {&#39;G_GAN&#39;: 6.906657695770264, &#39;G_L1&#39;: 64.83211517333984, &#39;D&#39;: 0.20042195916175842, &#39;F&#39;: 0.0011972549837082624}
Epoch/total_steps/alpha-beta: 1/13700/1-1 {&#39;G_GAN&#39;: 8.664873123168945, &#39;G_L1&#39;: 48.157352447509766, &#39;D&#39;: 0.10982154309749603, &#39;F&#39;: 0.0012957439757883549}
Epoch/total_steps/alpha-beta: 1/13800/1-1 {&#39;G_GAN&#39;: 8.40473747253418, &#39;G_L1&#39;: 82.69070434570312, &#39;D&#39;: 0.22358351945877075, &#39;F&#39;: 0.0013648176100105047}
Epoch/total_steps/alpha-beta: 1/13900/1-1 {&#39;G_GAN&#39;: 6.874181270599365, &#39;G_L1&#39;: 26.418155670166016, &#39;D&#39;: 0.28939008712768555, &#39;F&#39;: 0.0012917763087898493}
Epoch/total_steps/alpha-beta: 1/14000/1-1 {&#39;G_GAN&#39;: 5.609704494476318, &#39;G_L1&#39;: 42.911773681640625, &#39;D&#39;: 0.634445309638977, &#39;F&#39;: 0.0018506052438169718}
Epoch/total_steps/alpha-beta: 1/14100/1-1 {&#39;G_GAN&#39;: 6.2971391677856445, &#39;G_L1&#39;: 58.57958984375, &#39;D&#39;: 0.5101325511932373, &#39;F&#39;: 0.0013105932157486677}
Epoch/total_steps/alpha-beta: 1/14200/1-1 {&#39;G_GAN&#39;: 8.584198951721191, &#39;G_L1&#39;: 65.98389434814453, &#39;D&#39;: 0.19911500811576843, &#39;F&#39;: 0.0007704279851168394}
Epoch/total_steps/alpha-beta: 1/14300/1-1 {&#39;G_GAN&#39;: 6.8249359130859375, &#39;G_L1&#39;: 47.252079010009766, &#39;D&#39;: 0.16706067323684692, &#39;F&#39;: 0.002328396774828434}
Epoch/total_steps/alpha-beta: 1/14400/1-1 {&#39;G_GAN&#39;: 8.38831901550293, &#39;G_L1&#39;: 51.8577766418457, &#39;D&#39;: 0.10940229147672653, &#39;F&#39;: 0.0012676790356636047}
Epoch/total_steps/alpha-beta: 1/14500/1-1 {&#39;G_GAN&#39;: 7.214712142944336, &#39;G_L1&#39;: 28.135412216186523, &#39;D&#39;: 0.12094077467918396, &#39;F&#39;: 0.0015316965291276574}
Epoch/total_steps/alpha-beta: 1/14600/1-1 {&#39;G_GAN&#39;: 8.174083709716797, &#39;G_L1&#39;: 97.63543701171875, &#39;D&#39;: 0.06311690807342529, &#39;F&#39;: 0.0018654705490916967}
Epoch/total_steps/alpha-beta: 1/14700/1-1 {&#39;G_GAN&#39;: 8.422822952270508, &#39;G_L1&#39;: 47.57001495361328, &#39;D&#39;: 0.049740761518478394, &#39;F&#39;: 0.0022448422387242317}
Epoch/total_steps/alpha-beta: 1/14800/1-1 {&#39;G_GAN&#39;: 9.991469383239746, &#39;G_L1&#39;: 102.05438232421875, &#39;D&#39;: 0.9361597299575806, &#39;F&#39;: 0.004633892327547073}
Epoch/total_steps/alpha-beta: 1/14900/1-1 {&#39;G_GAN&#39;: 6.368223190307617, &#39;G_L1&#39;: 49.527645111083984, &#39;D&#39;: 0.3444039821624756, &#39;F&#39;: 0.0014356159372255206}
Epoch/total_steps/alpha-beta: 1/15000/1-1 {&#39;G_GAN&#39;: 7.705367088317871, &#39;G_L1&#39;: 28.593271255493164, &#39;D&#39;: 0.029679305851459503, &#39;F&#39;: 0.0070795160718262196}
Epoch/total_steps/alpha-beta: 1/15100/1-1 {&#39;G_GAN&#39;: 7.957377910614014, &#39;G_L1&#39;: 80.0302963256836, &#39;D&#39;: 0.04116593301296234, &#39;F&#39;: 0.0015837373211979866}
Epoch/total_steps/alpha-beta: 1/15200/1-1 {&#39;G_GAN&#39;: 7.570688724517822, &#39;G_L1&#39;: 39.492366790771484, &#39;D&#39;: 0.04144105315208435, &#39;F&#39;: 0.0016224775463342667}
Epoch/total_steps/alpha-beta: 1/15300/1-1 {&#39;G_GAN&#39;: 7.627815246582031, &#39;G_L1&#39;: 51.580726623535156, &#39;D&#39;: 0.04008587449789047, &#39;F&#39;: 0.006936891470104456}
Epoch/total_steps/alpha-beta: 1/15400/1-1 {&#39;G_GAN&#39;: 8.60068130493164, &#39;G_L1&#39;: 67.5638656616211, &#39;D&#39;: 0.09305613487958908, &#39;F&#39;: 0.0019613392651081085}
Epoch/total_steps/alpha-beta: 1/15500/1-1 {&#39;G_GAN&#39;: 7.0132036209106445, &#39;G_L1&#39;: 43.54671096801758, &#39;D&#39;: 0.15567710995674133, &#39;F&#39;: 0.0020579372067004442}
Epoch/total_steps/alpha-beta: 1/15600/1-1 {&#39;G_GAN&#39;: 7.285982131958008, &#39;G_L1&#39;: 59.41569137573242, &#39;D&#39;: 0.09708955138921738, &#39;F&#39;: 0.00134210754185915}
Epoch/total_steps/alpha-beta: 1/15700/1-1 {&#39;G_GAN&#39;: 5.872897148132324, &#39;G_L1&#39;: 57.65104293823242, &#39;D&#39;: 0.5190733671188354, &#39;F&#39;: 0.0014739809557795525}
Epoch/total_steps/alpha-beta: 1/15800/1-1 {&#39;G_GAN&#39;: 6.652884006500244, &#39;G_L1&#39;: 69.0383529663086, &#39;D&#39;: 0.2209867238998413, &#39;F&#39;: 0.000668725639116019}
Epoch/total_steps/alpha-beta: 1/15900/1-1 {&#39;G_GAN&#39;: 8.922956466674805, &#39;G_L1&#39;: 74.76789855957031, &#39;D&#39;: 0.14734676480293274, &#39;F&#39;: 0.001433783327229321}
Epoch/total_steps/alpha-beta: 1/16000/1-1 {&#39;G_GAN&#39;: 6.722776412963867, &#39;G_L1&#39;: 47.736106872558594, &#39;D&#39;: 0.19235579669475555, &#39;F&#39;: 0.004605707712471485}
Epoch/total_steps/alpha-beta: 1/16100/1-1 {&#39;G_GAN&#39;: 7.069607257843018, &#39;G_L1&#39;: 64.81068420410156, &#39;D&#39;: 0.13424471020698547, &#39;F&#39;: 0.0014468077570199966}
Epoch/total_steps/alpha-beta: 1/16200/1-1 {&#39;G_GAN&#39;: 5.65114164352417, &#39;G_L1&#39;: 46.55643081665039, &#39;D&#39;: 0.6477416753768921, &#39;F&#39;: 0.001036064699292183}
Epoch/total_steps/alpha-beta: 1/16300/1-1 {&#39;G_GAN&#39;: 5.594830513000488, &#39;G_L1&#39;: 41.98318862915039, &#39;D&#39;: 0.8094563484191895, &#39;F&#39;: 0.0020123659633100033}
Epoch/total_steps/alpha-beta: 1/16400/1-1 {&#39;G_GAN&#39;: 7.108182907104492, &#39;G_L1&#39;: 72.10615539550781, &#39;D&#39;: 0.1686408519744873, &#39;F&#39;: 0.0013924853410571814}
Epoch/total_steps/alpha-beta: 1/16500/1-1 {&#39;G_GAN&#39;: 6.439988613128662, &#39;G_L1&#39;: 36.351566314697266, &#39;D&#39;: 0.25065937638282776, &#39;F&#39;: 0.0011856399942189455}
Epoch/total_steps/alpha-beta: 1/16600/1-1 {&#39;G_GAN&#39;: 6.249668121337891, &#39;G_L1&#39;: 51.77361297607422, &#39;D&#39;: 1.8064351081848145, &#39;F&#39;: 0.0011435834458097816}
Epoch/total_steps/alpha-beta: 1/16700/1-1 {&#39;G_GAN&#39;: 7.325984001159668, &#39;G_L1&#39;: 52.518409729003906, &#39;D&#39;: 0.07044397294521332, &#39;F&#39;: 0.0017613430973142385}
Epoch/total_steps/alpha-beta: 1/16800/1-1 {&#39;G_GAN&#39;: 7.064994812011719, &#39;G_L1&#39;: 51.93901062011719, &#39;D&#39;: 0.10447141528129578, &#39;F&#39;: 0.0034984704107046127}
Epoch/total_steps/alpha-beta: 1/16900/1-1 {&#39;G_GAN&#39;: 8.45097541809082, &#39;G_L1&#39;: 60.183685302734375, &#39;D&#39;: 0.040800079703330994, &#39;F&#39;: 0.000756916357204318}
Epoch/total_steps/alpha-beta: 1/17000/1-1 {&#39;G_GAN&#39;: 7.472530841827393, &#39;G_L1&#39;: 68.05380249023438, &#39;D&#39;: 0.0651281476020813, &#39;F&#39;: 0.0009689441649243236}
Epoch/total_steps/alpha-beta: 1/17100/1-1 {&#39;G_GAN&#39;: 5.831203460693359, &#39;G_L1&#39;: 67.09069061279297, &#39;D&#39;: 0.8143503665924072, &#39;F&#39;: 0.0007786196074448526}
Epoch/total_steps/alpha-beta: 1/17200/1-1 {&#39;G_GAN&#39;: 7.75128173828125, &#39;G_L1&#39;: 33.496116638183594, &#39;D&#39;: 0.025084853172302246, &#39;F&#39;: 0.0016133107710629702}
Epoch/total_steps/alpha-beta: 1/17300/1-1 {&#39;G_GAN&#39;: 7.38450813293457, &#39;G_L1&#39;: 52.769134521484375, &#39;D&#39;: 0.1037171483039856, &#39;F&#39;: 0.001409650663845241}
Epoch/total_steps/alpha-beta: 1/17400/1-1 {&#39;G_GAN&#39;: 7.340816020965576, &#39;G_L1&#39;: 40.84172058105469, &#39;D&#39;: 0.11314868927001953, &#39;F&#39;: 0.0037464790511876345}
Epoch/total_steps/alpha-beta: 1/17500/1-1 {&#39;G_GAN&#39;: 8.718326568603516, &#39;G_L1&#39;: 81.49516296386719, &#39;D&#39;: 0.24362292885780334, &#39;F&#39;: 0.0011034500785171986}
Epoch/total_steps/alpha-beta: 1/17600/1-1 {&#39;G_GAN&#39;: 7.485352516174316, &#39;G_L1&#39;: 64.78911590576172, &#39;D&#39;: 0.07829080522060394, &#39;F&#39;: 0.0015187687240540981}
Epoch/total_steps/alpha-beta: 1/17700/1-1 {&#39;G_GAN&#39;: 7.694465637207031, &#39;G_L1&#39;: 81.69522857666016, &#39;D&#39;: 0.023307785391807556, &#39;F&#39;: 0.002601022832095623}
Epoch/total_steps/alpha-beta: 1/17800/1-1 {&#39;G_GAN&#39;: 7.043097496032715, &#39;G_L1&#39;: 46.4925651550293, &#39;D&#39;: 0.12455454468727112, &#39;F&#39;: 0.002275500912219286}
Epoch/total_steps/alpha-beta: 1/17900/1-1 {&#39;G_GAN&#39;: 7.806692600250244, &#39;G_L1&#39;: 38.93135452270508, &#39;D&#39;: 0.026306627318263054, &#39;F&#39;: 0.006160964723676443}
Epoch/total_steps/alpha-beta: 1/18000/1-1 {&#39;G_GAN&#39;: 7.079763889312744, &#39;G_L1&#39;: 31.32964324951172, &#39;D&#39;: 0.1718442142009735, &#39;F&#39;: 0.0032081922981888056}
Epoch/total_steps/alpha-beta: 1/18100/1-1 {&#39;G_GAN&#39;: 7.562501430511475, &#39;G_L1&#39;: 58.454586029052734, &#39;D&#39;: 0.06112927198410034, &#39;F&#39;: 0.0009278167854063213}
Epoch/total_steps/alpha-beta: 1/18200/1-1 {&#39;G_GAN&#39;: 5.266758918762207, &#39;G_L1&#39;: 43.96367263793945, &#39;D&#39;: 0.99897700548172, &#39;F&#39;: 0.003405321389436722}
Epoch/total_steps/alpha-beta: 1/18300/1-1 {&#39;G_GAN&#39;: 8.84471607208252, &#39;G_L1&#39;: 38.407100677490234, &#39;D&#39;: 0.09829668700695038, &#39;F&#39;: 0.0013083667727187276}
Epoch/total_steps/alpha-beta: 1/18400/1-1 {&#39;G_GAN&#39;: 7.709201812744141, &#39;G_L1&#39;: 53.043670654296875, &#39;D&#39;: 0.11290895938873291, &#39;F&#39;: 0.0016015595756471157}
Epoch/total_steps/alpha-beta: 1/18500/1-1 {&#39;G_GAN&#39;: 6.139512062072754, &#39;G_L1&#39;: 90.53040313720703, &#39;D&#39;: 0.38660091161727905, &#39;F&#39;: 0.001467687077820301}
Epoch/total_steps/alpha-beta: 1/18600/1-1 {&#39;G_GAN&#39;: 5.867817401885986, &#39;G_L1&#39;: 57.414764404296875, &#39;D&#39;: 0.80483478307724, &#39;F&#39;: 0.0009237489430233836}
Epoch/total_steps/alpha-beta: 1/18700/1-1 {&#39;G_GAN&#39;: 8.204290390014648, &#39;G_L1&#39;: 52.65342712402344, &#39;D&#39;: 0.024031933397054672, &#39;F&#39;: 0.0017552648205310106}
Epoch/total_steps/alpha-beta: 1/18800/1-1 {&#39;G_GAN&#39;: 7.551031112670898, &#39;G_L1&#39;: 29.79721450805664, &#39;D&#39;: 0.05261358618736267, &#39;F&#39;: 0.013824671506881714}
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">6</span><span class="o">-</span><span class="mi">54</span><span class="n">fd66e4be6a</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">18</span>         <span class="n">model</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span>         <span class="n">model</span><span class="o">.</span><span class="n">set_gt_latent</span><span class="p">()</span>
<span class="ne">---&gt; </span><span class="mi">20</span>         <span class="n">model</span><span class="o">.</span><span class="n">optimize_parameters</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">21</span>         <span class="k">if</span> <span class="n">total_steps</span> <span class="o">%</span> <span class="n">display_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">22</span>             <span class="n">real_A</span><span class="p">,</span> <span class="n">real_B</span><span class="p">,</span> <span class="n">fake_B</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_current_visuals</span><span class="p">()</span>

<span class="nn">e:\kaggle\pytorch-book\apps\models\CSA.py</span> in <span class="ni">optimize_parameters</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">262</span> 
<span class="g g-Whitespace">    </span><span class="mi">263</span>     <span class="k">def</span> <span class="nf">optimize_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">264</span>         <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">265</span>         <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_D</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">266</span>         <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_F</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="nn">e:\kaggle\pytorch-book\apps\models\CSA.py</span> in <span class="ni">forward</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">178</span>         <span class="bp">self</span><span class="o">.</span><span class="n">Syn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Unknowregion</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">knownregion</span>
<span class="g g-Whitespace">    </span><span class="mi">179</span>         <span class="bp">self</span><span class="o">.</span><span class="n">Middle</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">Syn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_A</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">180</span>         <span class="bp">self</span><span class="o">.</span><span class="n">fake_B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">netG</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Middle</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">181</span>         <span class="bp">self</span><span class="o">.</span><span class="n">real_B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_B</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">182</span> 

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">e:\kaggle\pytorch-book\apps\models\networks.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">162</span> 
<span class="g g-Whitespace">    </span><span class="mi">163</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">164</span>         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">165</span> 
<span class="g g-Whitespace">    </span><span class="mi">166</span> 

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">e:\kaggle\pytorch-book\apps\models\utils.py</span> in <span class="ni">forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">    </span><span class="mi">174</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">175</span>         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outermost</span><span class="p">:</span>  <span class="c1"># if it is the outermost, directly pass the input in.</span>
<span class="ne">--&gt; </span><span class="mi">176</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">177</span>         <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">178</span>             <span class="n">x_latter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\container.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">139</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">140</span>         <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">141</span>             <span class="nb">input</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">142</span>         <span class="k">return</span> <span class="nb">input</span>
<span class="g g-Whitespace">    </span><span class="mi">143</span> 

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">e:\kaggle\pytorch-book\apps\models\utils.py</span> in <span class="ni">forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">    </span><span class="mi">176</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">177</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">178</span>             <span class="n">x_latter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">179</span>             <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">180</span>             <span class="k">if</span> <span class="n">h</span> <span class="o">!=</span> <span class="n">x_latter</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="ow">or</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">x_latter</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\container.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">139</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">140</span>         <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">141</span>             <span class="nb">input</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">142</span>         <span class="k">return</span> <span class="nb">input</span>
<span class="g g-Whitespace">    </span><span class="mi">143</span> 

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">e:\kaggle\pytorch-book\apps\models\utils.py</span> in <span class="ni">forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">    </span><span class="mi">176</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">177</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">178</span>             <span class="n">x_latter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">179</span>             <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">180</span>             <span class="k">if</span> <span class="n">h</span> <span class="o">!=</span> <span class="n">x_latter</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="ow">or</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">x_latter</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\container.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">139</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">140</span>         <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">141</span>             <span class="nb">input</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">142</span>         <span class="k">return</span> <span class="nb">input</span>
<span class="g g-Whitespace">    </span><span class="mi">143</span> 

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">e:\kaggle\pytorch-book\apps\models\networks.py</span> in <span class="ni">forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">    </span><span class="mi">128</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">129</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">130</span>             <span class="n">x_latter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">131</span>             <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">132</span>             <span class="k">if</span> <span class="n">h</span> <span class="o">!=</span> <span class="n">x_latter</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="ow">or</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">x_latter</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\container.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">139</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">140</span>         <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">141</span>             <span class="nb">input</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">142</span>         <span class="k">return</span> <span class="nb">input</span>
<span class="g g-Whitespace">    </span><span class="mi">143</span> 

<span class="nn">~\.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">e:\kaggle\pytorch-book\apps\models\CSA_model.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">     </span><span class="mi">50</span>                 <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">51</span> 
<span class="ne">---&gt; </span><span class="mi">52</span>         <span class="k">return</span> <span class="n">CSAFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shift_sz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">53</span>                                  <span class="bp">self</span><span class="o">.</span><span class="n">triple_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">flag</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonmask_point_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_point_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten_offsets</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">54</span>                                  <span class="bp">self</span><span class="o">.</span><span class="n">sp_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sp_y</span><span class="p">)</span>

<span class="nn">e:\kaggle\pytorch-book\apps\models\CSAFunction.py</span> in <span class="ni">forward</span><span class="nt">(ctx, input, mask, shift_sz, stride, triple_w, flag, nonmask_point_idx, mask_point_idx, flatten_offsets, sp_x, sp_y)</span>
<span class="g g-Whitespace">     </span><span class="mi">40</span> 
<span class="g g-Whitespace">     </span><span class="mi">41</span>             <span class="n">Nonparm</span> <span class="o">=</span> <span class="n">NonparametricShift</span><span class="p">()</span>
<span class="ne">---&gt; </span><span class="mi">42</span>             <span class="n">_</span><span class="p">,</span> <span class="n">conv_enc</span><span class="p">,</span> <span class="n">conv_new_dec</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">known_patch</span><span class="p">,</span> <span class="n">unknown_patch</span> <span class="o">=</span> <span class="n">Nonparm</span><span class="o">.</span><span class="n">buildAutoencoder</span><span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">43</span>                 <span class="n">inpatch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">nonmask_point_idx</span><span class="p">,</span> <span class="n">mask_point_idx</span><span class="p">,</span>  <span class="n">shift_sz</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">44</span> 

<span class="nn">e:\kaggle\pytorch-book\apps\util\NonparametricShift.py</span> in <span class="ni">buildAutoencoder</span><span class="nt">(self, target_img, normalize, interpolate, nonmask_point_idx, mask_point_idx, patch_size, stride)</span>
<span class="g g-Whitespace">     </span><span class="mi">23</span>         <span class="n">conv_enc_non_mask</span><span class="p">,</span> <span class="n">conv_dec_non_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">patches_part</span><span class="p">,</span> <span class="n">npatches_part</span><span class="p">,</span> <span class="n">normalize</span><span class="p">,</span> <span class="n">interpolate</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">24</span> 
<span class="ne">---&gt; </span><span class="mi">25</span>         <span class="n">conv_enc_all</span><span class="p">,</span> <span class="n">conv_dec_all</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">patches_all</span><span class="p">,</span> <span class="n">npatches_all</span><span class="p">,</span> <span class="n">normalize</span><span class="p">,</span> <span class="n">interpolate</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">26</span> 
<span class="g g-Whitespace">     </span><span class="mi">27</span> 

<span class="nn">e:\kaggle\pytorch-book\apps\util\NonparametricShift.py</span> in <span class="ni">_build</span><span class="nt">(self, patch_size, stride, C, target_patches, npatches, normalize, interpolate)</span>
<span class="g g-Whitespace">     </span><span class="mi">33</span>         <span class="n">enc_patches</span> <span class="o">=</span> <span class="n">target_patches</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">34</span>         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">npatches</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">35</span>             <span class="n">enc_patches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">enc_patches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">enc_patches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">+</span><span class="mf">1e-8</span><span class="p">))</span>
<span class="g g-Whitespace">     </span><span class="mi">36</span> 
<span class="g g-Whitespace">     </span><span class="mi">37</span>         <span class="n">conv_enc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">npatches</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "SanstyleLab/pytorch-book",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./use"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By xinetzone<br/>
        
          <div class="extra_footer">
            <p class="w3-card w3-pale-blue w3-padding">
  Copyright¬†¬©¬†2021
  <a href="https://sanstylelab.github.io/">SanstyleLab</a>¬†|¬†
  Powered by¬†<a href="https://github.com/executablebooks/jupyter-book">Jupyter Book</a>.
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>